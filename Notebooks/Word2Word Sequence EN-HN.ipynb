{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Translation English to Hidni\n",
    "\n",
    "This notebook presents a machine translation model using word to word sequence using an LSTM neural Network the dataset used in the note book was taken from http://www.manythings.org/anki/hin-eng.zip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import time\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import shuffle\n",
    "import unicodedata\n",
    "import warnings\n",
    "from nltk import word_tokenize\n",
    "import keras \n",
    "import keras.backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM, Dense, SpatialDropout1D\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reading, Cleaning  and transformation for both Languages\n",
    "\n",
    "In order to make this data suitable for our model we need to do some data cleaning and transformation such as\n",
    "\n",
    "**Read the data**:<br> \n",
    "1. Read the data from file\n",
    "\n",
    "**Data Cleaning**:<br> \n",
    "1. Remove the un-recognized and special characters<br>\n",
    "2. Drop words containing non-alphabetical words\n",
    "\n",
    "**Transformation**:<br>\n",
    "1. Normalize the unicode encoding for transforming the characters to same representation\n",
    "2. Change the text to lower case for reducing the capitalization overhead\n",
    "3. Tokenize the texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read and process the data**\n",
    "\n",
    "We will read the data and then perform required pre-processing such as tokenization and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(filename, encoding='utf-8', normalize=False, tokenize=True):\n",
    "    file = open(filename, 'r',encoding=encoding)\n",
    "    data = file.readlines()\n",
    "    file.close()\n",
    "    src_text = []\n",
    "    trg_text = []\n",
    "    for line in data:\n",
    "        #Split the text to seperate source and target text\n",
    "        text_array = line.split(\"\\t\")\n",
    "        src = text_array[0]\n",
    "        trg = text_array[1].strip()\n",
    "        if normalize:\n",
    "            if tokenize:\n",
    "                eng = nltk.word_tokenize(unicodedata.normalize('NFD', src).encode('ascii', 'ignore').decode('UTF-8'))\n",
    "                fra = nltk.word_tokenize(unicodedata.normalize('NFD', trg).encode('ascii', 'ignore').decode('UTF-8'))\n",
    "            else:\n",
    "                eng = nltk.word_tokenize(unicodedata.normalize('NFD', src).encode('ascii', 'ignore').decode('UTF-8'))\n",
    "                fra = nltk.word_tokenize(unicodedata.normalize('NFD', trg).encode('ascii', 'ignore').decode('UTF-8'))\n",
    "        else:\n",
    "            if normalize:\n",
    "                eng = unicodedata.normalize('NFD', src).encode('ascii', 'ignore').decode('UTF-8')\n",
    "                fra = unicodedata.normalize('NFD', trg).encode('ascii', 'ignore').decode('UTF-8')\n",
    "            elif tokenize:\n",
    "                eng = nltk.word_tokenize(src)\n",
    "                fra = nltk.word_tokenize(trg)\n",
    "                \n",
    "        src_text.append(eng)\n",
    "        trg_text.append(trg)\n",
    "        \n",
    "    return src_text, trg_text\n",
    "src_txt, trg_text = read_text('hin.txt', encoding='utf-8', normalize=True, tokenize=True)\n",
    "text = pd.DataFrame({\"src\":src_txt,\"target\":trg_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Partitioning**\n",
    "\n",
    "We need to split the data for training the model and then evaluating the  mode on the sample data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = text\n",
    "shuffle(data.values)\n",
    "train = data[0:int(data.shape[0]*0.9)]\n",
    "test = data[int(data.shape[0]*0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions**\n",
    "\n",
    "We will define some helper function to perform processing like mapping text to features and doing one-hot-encoding on the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_features(tokenizer, texts, max_length):    \n",
    "    \"\"\"\n",
    "    The function maps the text to feature vectors based on the  \n",
    "    :param tokenizer: Tokenizer for the given laguage \n",
    "    :param text: text to map into feature vectors\n",
    "    :param max_length: longest text length\n",
    "    \"\"\"\n",
    "    feature_vectors = tokenizer.texts_to_sequences(np.array(texts, dtype=object))\n",
    "    feature_vectors = pad_sequences(feature_vectors, maxlen=max_length, padding='post')\n",
    "    return feature_vectors\n",
    "\n",
    "def tokenizer(text, max_num_words=None):\n",
    "    \"\"\"\n",
    "    The function fits a keras tokenizer on the text\n",
    "    :param text: text to tokenize\n",
    "    :param max_num_words: maximum number of words to consider\n",
    "    \"\"\"\n",
    "    if max_num_words == None:\n",
    "        tokenizer = Tokenizer()\n",
    "    else:\n",
    "        tokenizer = Tokenizer(num_words=max_num_words)\n",
    "    tokenizer.fit_on_texts(np.array(text, dtype=object))\n",
    "    return tokenizer\n",
    "\n",
    "def one_hot_encode(target_feature_vectors, vocab_size):\n",
    "    \"\"\"\n",
    "    This methods encodes the features into one-hot encoding\n",
    "    :param target_feature_vectors: feature vectors to encode\n",
    "    :param vocab_size: size of the vocabulary\n",
    "    \"\"\"\n",
    "    one_hot_encoded_target = []\n",
    "    for i in range(target_feature_vectors.shape[0]):\n",
    "        one_hot_encoded_target.append(keras.utils.to_categorical(target_feature_vectors[i], num_classes=vocab_size))\n",
    "    return np.array(one_hot_encoded_target)\n",
    "    \n",
    "def text_to_sequence(tokenizer, texts, length):\n",
    "    \"\"\"\n",
    "    text_to_sequence maps the text to a sequence of numbers using keras tokenizer\n",
    "    :param target_feature_vectors: feature vectors to encode\n",
    "    :param vocab_size: size of the vocabulary\n",
    "    \"\"\"\n",
    "    # integer encode sequences\n",
    "    if type(texts) == pd.Series:\n",
    "        X = tokenizer.texts_to_sequences(np.array(texts))\n",
    "    else:\n",
    "        X = tokenizer.texts_to_sequences(np.array([texts]))\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Mapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = tokenizer(train['src'], max_num_words=500)\n",
    "src_max_text_length = max(train['src'].apply(len))\n",
    "src_vocab_size = len(src_tokenizer.word_index)+1 \n",
    "\n",
    "target_tokenizer = tokenizer(train['target'], max_num_words=500)\n",
    "target_max_text_length = max(train['target'].apply(len))\n",
    "target_vocab_size = len(target_tokenizer.word_index)+1 \n",
    "\n",
    "train_X = map_to_features(src_tokenizer, train['src'], src_max_text_length)\n",
    "train_Y = map_to_features(target_tokenizer, train['target'], target_max_text_length)\n",
    "train_Y_labels = one_hot_encode(train_Y, len(target_tokenizer.word_index)+1)\n",
    "\n",
    "test_X = map_to_features(src_tokenizer, test['src'], src_max_text_length)\n",
    "test_Y = map_to_features(target_tokenizer, test['target'], target_max_text_length)\n",
    "test_Y_labels = one_hot_encode(test_Y, len(target_tokenizer.word_index)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 26, 128)           289920    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 121, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 121, 64)           33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 121, 2852)         185380    \n",
      "=================================================================\n",
      "Total params: 557,732\n",
      "Trainable params: 557,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_units =64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(src_vocab_size, embed_dim, input_length=src_max_text_length, mask_zero=True))\n",
    "model.add(LSTM(lstm_units))\n",
    "model.add(RepeatVector(target_max_text_length))\n",
    "model.add(LSTM(lstm_units, return_sequences=True))\n",
    "model.add(Dense(target_vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2579 samples, validate on 287 samples\n",
      "Epoch 1/20\n",
      "2579/2579 [==============================] - 89s 34ms/step - loss: 1.1644 - val_loss: 0.2873\n",
      "Epoch 2/20\n",
      "2579/2579 [==============================] - 89s 35ms/step - loss: 0.2738 - val_loss: 0.2567\n",
      "Epoch 3/20\n",
      "2579/2579 [==============================] - 86s 33ms/step - loss: 0.2601 - val_loss: 0.2588\n",
      "Epoch 4/20\n",
      "2579/2579 [==============================] - 87s 34ms/step - loss: 0.2546 - val_loss: 0.2503\n",
      "Epoch 5/20\n",
      "2579/2579 [==============================] - 89s 34ms/step - loss: 0.2511 - val_loss: 0.2523\n",
      "Epoch 6/20\n",
      "2579/2579 [==============================] - 86s 33ms/step - loss: 0.2487 - val_loss: 0.2460\n",
      "Epoch 7/20\n",
      "2579/2579 [==============================] - 87s 34ms/step - loss: 0.2470 - val_loss: 0.2448\n",
      "Epoch 8/20\n",
      "2579/2579 [==============================] - 86s 33ms/step - loss: 0.2454 - val_loss: 0.2446\n",
      "Epoch 9/20\n",
      "2579/2579 [==============================] - 84s 33ms/step - loss: 0.2441 - val_loss: 0.2449\n",
      "Epoch 10/20\n",
      "2579/2579 [==============================] - 86s 33ms/step - loss: 0.2430 - val_loss: 0.2452\n",
      "Epoch 11/20\n",
      "2579/2579 [==============================] - 84s 33ms/step - loss: 0.2418 - val_loss: 0.2454\n",
      "Epoch 12/20\n",
      "2579/2579 [==============================] - 85s 33ms/step - loss: 0.2407 - val_loss: 0.2484\n",
      "Epoch 13/20\n",
      "2579/2579 [==============================] - 84s 33ms/step - loss: 0.2392 - val_loss: 0.2428\n",
      "Epoch 14/20\n",
      "2579/2579 [==============================] - 84s 33ms/step - loss: 0.2375 - val_loss: 0.2420\n",
      "Epoch 15/20\n",
      "2579/2579 [==============================] - 85s 33ms/step - loss: 0.2350 - val_loss: 0.2431\n",
      "Epoch 16/20\n",
      "2579/2579 [==============================] - 84s 33ms/step - loss: 0.2333 - val_loss: 0.2399\n",
      "Epoch 17/20\n",
      "2579/2579 [==============================] - 84s 33ms/step - loss: 0.2315 - val_loss: 0.2412\n",
      "Epoch 18/20\n",
      "2579/2579 [==============================] - 85s 33ms/step - loss: 0.2299 - val_loss: 0.2392\n",
      "Epoch 19/20\n",
      "2579/2579 [==============================] - 89s 35ms/step - loss: 0.2283 - val_loss: 0.2392\n",
      "Epoch 20/20\n",
      "2579/2579 [==============================] - 85s 33ms/step - loss: 0.2266 - val_loss: 0.2430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18e04b78e10>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=16\n",
    "model.fit(train_X, train_Y_labels, epochs=20, batch_size=batch_size, validation_data=(test_X, test_Y_labels), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text,source_tokenizer,source_text_length, target_tokenizer, target_text_length):\n",
    "    \"\"\"\n",
    "    This methods translate the text using the trained model\n",
    "    :param text: Source text to translate\n",
    "    :param source_tokenizer : Source language tokenizer \n",
    "    :paramsource_text_length: Longest text length in source language\n",
    "    :param target_tokenizer : Target language tokenizer \n",
    "    :param target_text_length: Longest text length in target language\n",
    "    :return translated_text \n",
    "    \"\"\"\n",
    "    features = text_to_sequence(source_tokenizer, text, source_text_length)\n",
    "    pred=model.predict(features)\n",
    "    max_probable_word_indicies = np.argmax(pred, axis=2)\n",
    "    translated_texts = []\n",
    "    for i in range (max_probable_word_indicies.shape[0]):\n",
    "        translated_text = \"\"\n",
    "        for j in range(max_probable_word_indicies.shape[1]):\n",
    "            for word, index in target_tokenizer.word_index.items():\n",
    "                if max_probable_word_indicies[i,j] == index:\n",
    "                    translated_text+=\" \"+word\n",
    "        translated_texts.append(translated_text)\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Target Text= वह में के के से से\t\t Predicted Test=इस समस्या की तहकीकात करने के लिए एक समिति स्थापित करी गई है।\n",
      "Actual Target Text= मैं में में में में में में से\t\t Predicted Test=मेरी ट्रेन में एक पुराने दोस्त से मुलाक़ात हुई।\n",
      "\n",
      "Average BLEU Score on training for 10 random Text = 0.271\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(13)\n",
    "weights=(1.0, 0, 0, 0)\n",
    "index = np.random.choice(train['src'].shape[0], size=count,replace=False)\n",
    "text = train['src'][index].reset_index(drop=True)\n",
    "predicted_text = translate(text,src_tokenizer, src_max_text_length, target_tokenizer, target_max_text_length)\n",
    "target_text = list(train['target'][index])\n",
    "bleu_s = 0\n",
    "for i in range(len(index)):\n",
    "    #Just to display last 3 translation\n",
    "    if i>7:\n",
    "        print(\"Actual Target Text=\"+predicted_text[i]+\"\\t\\t Predicted Test=\"+target_text[i])\n",
    "    bleu_s+=nltk.translate.bleu_score.sentence_bleu([predicted_text[i]], target_text[i], weights=weights)\n",
    "bleu_s = bleu_s/10\n",
    "print(\"\\nAverage BLEU Score on training for 10 random Text = %.3f\"% bleu_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Target Text= वह में के के से से\t\t Predicted Test=इस समस्या की तहकीकात करने के लिए एक समिति स्थापित करी गई है।\n",
      "Actual Target Text= मैं में में में में में में से\t\t Predicted Test=मेरी ट्रेन में एक पुराने दोस्त से मुलाक़ात हुई।\n",
      "\n",
      "Average BLEU Score on training for 10 random Text = 0.161\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(13)\n",
    "weights=(0.5, 0.5, 0, 0)\n",
    "index = np.random.choice(train['src'].shape[0], size=count,replace=False)\n",
    "text = train['src'][index].reset_index(drop=True)\n",
    "predicted_text = translate(text,src_tokenizer, src_max_text_length, target_tokenizer, target_max_text_length)\n",
    "target_text = list(train['target'][index])\n",
    "bleu_s = 0\n",
    "for i in range(len(index)):\n",
    "    #Just to display last 3 translation\n",
    "    if i>7:\n",
    "        print(\"Actual Target Text=\"+predicted_text[i]+\"\\t\\t Predicted Test=\"+target_text[i])\n",
    "    bleu_s+=nltk.translate.bleu_score.sentence_bleu([predicted_text[i]], target_text[i], weights=weights)\n",
    "bleu_s = bleu_s/10\n",
    "print(\"\\nAverage BLEU Score on training for 10 random Text = %.3f\"% bleu_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**\n",
    "1-gram Bleu score is 0.27 and 2-gram Bleu score is 0.16 which is not a satisfactory score and the translations are not good, It might need extra training aur a better more data as a good data is also a problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2word_hin.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
